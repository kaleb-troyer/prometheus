# run srlife sim for the first time
#
# Specify the HTCondor Universe (vanilla is the default and is used
#  for almost all jobs) and your desired name of the HTCondor log file,
#  which is where HTCondor will describe what steps it takes to run 
#  your job. Wherever you see $(Cluster), HTCondor will insert the 
#  queue number assigned to this set of jobs at the time of submission.
universe = vanilla
log = logs/$(run).log
#
# Specify your executable (single binary or a script that runs several
#  commands), arguments, and a files for HTCondor to store standard
#  output (or "screen output").
#  $(Process) will be a integer number for each job, starting with "0"
#  and increasing for the relevant number of jobs.
executable = srlifeRunAllDeltaTs.sh
arguments = $(run)
# removed $(Process) from arguments line
output = output/$(run).out
error = errors/$(run).err
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs. The last of these lines *would* be
#  used if there were any other files needed for the executable to use.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT


# transfer_input_files = http://proxy.chtc.wisc.edu/SQUID/chtc/el8/python38.tar.gz, packages38_rev1.tar.gz, $(run).hdf5, chtc_main_P1.py #this didn't work
# transfer_input_files = packages38_rev1.tar.gz, $(run).hdf5, chtc_main_P1.py	# this didn't work
transfer_input_files = $(run).hdf5, chtc_main_P2.py
# transfer_output_remaps = "$(run)_solved.hdf5=output/$(run)_solved.hdf5"
# transfer_output_files = A230_$(run)_solved.hdf5
transfer_output_files = $(run)_with_A230_results.csv

# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.

requirements = (HasCHTCStaging == true)
container_image = file:///staging/bepagel/srlife-container2.sif
# chtc_want_el8 = true

request_cpus = 1
# changed to 1 because code logs indicate only using 1 core
request_memory = 30GB
# 60 seems more than adequate for big 200+ substep 3 day jobs
# used 40 for 80 substep 3 day jobs, worked
# 30 GB was adequate for all 3day 60 substep cosTimeFrac simulations, which maxed out at 24 GB
request_disk = 10GB

# used 10 GB for 3 day 80 substep jobs, worked
# used 20 GB for disk for bigger stuff. No issues yet
#
# Tell HTCondor to run all instances of our job from my list:
queue run from modelList.txt
